# Table of Contents

- Overview and Goals
- Branching Strategy and Flow
- Content Data Model / Frontmatter Schema
- Core Scripts
  - 4.1 Translation Task Detection
  - 4.2 Translation + TLDR + TextScore Generation
  - 4.3 Quality Review & Regeneration
- Prompt Design
- GitHub Actions Workflows
  - 6.1 main → translate branch action
  - 6.2 Manual review & promotion to release
  - 6.3 post-release feedback to main
  - 6.4 Bad translation detection & issue creation
  - 6.5 Token usage tracking and throttling
- Conflict Resolution & Overrides
- Metrics, Logging, and Summary
- Naming Conventions and Governance
- Deployment & Release
- Security & Secret Hygiene
- Testing & Validation
- Step-by-step Implementation Checklist
- Reviewer / Operator Guidance
- Appendix: Example Branch/PR Titles and Commit Messages

---

## 1. Overview and Goals

This document captures the full implementation plan for a robust, reviewable, and cost-aware automatic translation pipeline using the OpenAI API (model: gpt-4.1-nano or gpt-4o) for an Astro/Astrowind-based content site. The pipeline achieves the following:

- Author content in a draft branch, promote to main when ready.
- Automatically detect missing or stale translations on main and generate translations (with TLDR and quality score) via a single combined OpenAI prompt.
- Surface AI-generated translations in a temporary translate branch as a draft PR, allowing human review/edits.
- Promote approved translations into the release branch via PR; only release is deployed to GitHub Pages.
- Feed back the finalized translation content into main after release.
- Provide translation provenance, automated TLDR and quality scoring, conflict resolution, override controls, and transparent metrics including token usage capped at 2 million tokens/day.
- Auto-detect poor translations and open Issues with actionable diffs.

This pipeline minimizes long-lived branch complexity, uses GitHub-native review gating, preserves provenance, and integrates SEO and fallback logic.

---

## 2. Branching Strategy and Flow

Branches serve distinct purposes; long-lived translation branches are avoided in favor of short-lived, per-batch temporary ones.

- **draft**: Developers write or iterate on new content here. This is the staging space before public readiness.
- **main**: Content is promoted from draft to main (e.g., via merge). Upon push to main, the translation pipeline runs.
- **translate/\* (temporary)**: Automatically created/updated branches with AI-generated translations (and TLDR/score), one per batch (naming convention described below). These are draft PRs targeting release for human review.
- **release**: Only this branch is deployed to GitHub Pages. Human-approved translations are merged here from the translate/... branches.
- **Feedback to main**: After a successful merge into release, the translated files (with updated metadata and provenance) are pushed back into main (automatically via CI) so that main stays in sync and future changes are based on the approved content.

**Key constraints:**

- Prevent multiple concurrent translation branches for the same source content by deduplicating based on translationKey, sourceSha, and targetLang.
- All AI-generated translation branches are ephemeral; once merged or superseded, they are cleaned up or closed.

---

## 3. Content Data Model / Frontmatter Schema

Every content file must include (or be normalized to include) the following frontmatter fields to support detection, provenance, TLDR, and quality scoring:

```yaml
---
title: '...'
lang: 'en' # or "de"
translationKey: 'unique-slug-or-id'
date: '2025-08-01'
draft: false
tags: [...]
# Translation / authoring status
status:
  authoring: 'Human' # Human | AI | AI+Human
  translation: null # null | AI | Human | AI+Human
# For translations:
original: '../en/my-article.md' # reference to source
translationHistory:
  - language: 'de'
    translator: 'AI (gpt-4.1-nano)'
    model: 'gpt-4.1-nano'
    sourceSha: '<sha-of-original-content>'
    timestamp: '2025-08-02T12:00:00Z'
    status: 'ai-translated' # ai-translated | human-reviewed | ai+human
    reviewer: null # github username if human touched it
# AI generated summary and quality metric
ai_tldr: 'Short 3-4 sentence summary generated by AI.'
ai_textscore:
  translationQuality: 0.0 # numeric score (e.g., 0-100) from AI reviewer
  originalClarity: 0.0 # optional score on the original text (for comparison)
  timestamp: '2025-08-02T12:00:00Z'
---
```

**Utility considerations:**

- translationKey ensures pairing across languages regardless of slug variations.
- sourceSha is a hash (e.g., SHA256) of the normalized source content (excluding mutable metadata) used to detect staleness.
- ai_tldr and ai_textscore are generated together to minimize redundant API calls.

---

## 4. Core Scripts

### 4.1 Translation Task Detection (check_translations.js)

Responsibilities:

- Crawl main content directories and parse frontmatter.
- For each content item, identify counterpart(s) in the other language(s) by matching translationKey.
- Detect:
  - Missing translation (no counterpart file exists).
  - Stale translation (existing translated file’s translationHistory[0].sourceSha differs from current source’s sourceSha).
- Respect override mechanism (see Section 7) to skip auto translation when flagged.
- Output a normalized JSON list of translation tasks, e.g.:

```json
[
  {
    "sourcePath": "src/content/lab/en/my-article.md",
    "targetLang": "de",
    "translationKey": "my-article",
    "reason": "missing",
    "sourceSha": "abc123..."
  }
]
```

- Also expose which translation branches/PRs already exist for each task so the workflow can dedupe (by querying GitHub API for existing open draft PRs naming scheme).

### 4.2 Translation + TLDR + TextScore Generation (generate_translations.js)

Responsibilities:

- For each task from detection:
  - Load source markdown and compute sourceSha.
  - Build a single combined prompt to request:
    - Translation into the target language (preserving structure).
    - A 3-4 sentence TLDR of the content.
    - A quality score (textscore) for both the original and translated content (e.g., 0-100), possibly involving a secondary internal comparison.
  - Call OpenAI gpt-4.1-nano (or gpt-4o) with low temperature (0.2–0.4) to get deterministic output.
  - Parse structured output (see Prompt Design) reliably into:
    - Full translated markdown with updated frontmatter.
    - ai_tldr and ai_textscore fields.
  - Validate: Ensure required frontmatter fields are present, language code updated, translationHistory entry has correct metadata, and scores are numeric and reasonable.
  - Cache successful translation keyed by (sourceSha, targetLang) to avoid retranslation if nothing changed.
  - Write or update the translated file at the appropriate path following existing content organization (e.g., src/content/de/<slug>.md).

### 4.3 Quality Review & Regeneration

Responsibilities:

- After the translation is created, optionally run a secondary AI review (or it can be folded into the initial prompt) that:
  - Compares original vs translation.
  - Flags problematic segments (e.g., missing meaning, mistranslation, hallucinations).
  - Returns a list of issues with context suitable for inclusion in a GitHub Issue.
- If the review indicates a “bad translation” (e.g., quality score below a threshold or flagged items exist), automatically create a GitHub Issue summarizing:
  - The translationKey and target language.
  - Specific problematic excerpts with minimal diff.
  - Suggested actions (e.g., regenerate with adjusted instructions or manual intervention).
- Support a regeneration policy: if a translation is marked for rework (via issue label or manual trigger), re-run generation, preserving the previous history and appending a new translationHistory entry with the updated model invocation.

---

## 5. Prompt Design

All AI interactions (translation + summary + scoring + review) derive from a single structured prompt pattern to reduce cost and complexity.

**Desired output format (machine-parseable):**

Return a JSON object with the following top-level keys:

```
{
  "translated_markdown": "...full markdown content...",
  "ai_tldr": "...3-4 sentence summary...",
  "ai_textscore": {
    "translationQuality": 0.0,
    "originalClarity": 0.0,
    "notes": ["optional explanations for scores"]
  },
  "review_issues": [
    {"section": "paragraph 2", "issue": "Potential mistranslation: ...", "suggestion": "...}
  ]
}
```

**Core instruction block (template):**

System: You are a precise translator, summarizer, and reviewer. Given a markdown document in <sourceLang>, perform all of the following in one pass:

1. Translate the human-readable content into <targetLang>, preserving all markdown structure (code fences, links, frontmatter keys except where updates are required). Do not alter URLs or variable tokens.
2. Produce a 3-4 sentence TLDR of the original document in the target language if translating, or in the source language if no translation is needed.
3. Evaluate the clarity of the original content and the quality of the translation; provide two numeric scores (0-100).
4. Compare the original vs translation to identify any problematic segments; for each, give a short description and suggestion.
5. Output exactly one JSON object with fields: translated_markdown, ai_tldr, ai_textscore, review_issues.

User: Here is the source document:
<insert full markdown>

Make sure the translation frontmatter inside translated_markdown reflects: updated lang, same translationKey, updated original reference, new translationHistory entry with model name gpt-4.1-nano, current timestamp, and statuses set appropriately.

---

## 6. GitHub Actions Workflows

### 6.1 main → translate branch action

Trigger: push to main.

Steps:

- Checkout the repo (full depth).
- Abort early if override flag is present for the affected translation tasks.
- Run check_translations.js to produce translation_tasks.json.
- Query the GitHub API to see if a draft PR already exists for each pending translation (keyed by translationKey, sourceSha, targetLang).
- If one exists: update its branch with new translation content.
- If none exists: run generate_translations.js to obtain translations, TLDR, scores, and draft a new branch.
- Create or update draft PR(s) from translate/<translationKey>-<shortSha>-<lang> into release with:
  - Title pattern: AI translations: <targetLang> for <translationKey> (source @ <shortSha>)
  - Body listing tasks, TLDRs, scores, and any flagged review issues.
  - Labels: ai-translation, needs-review, translation-stale if applicable.
  - Attach or surface in the PR summary the token usage for that run.

**Deduplication logic:**

- Use consistent branch naming. Before creating a new branch, search existing open draft PRs via GitHub REST or GraphQL API. If a matching one exists (same identifiers), reuse its branch and update contents instead of creating a parallel PR.

### 6.2 Manual review & promotion to release

- Reviewer inspects the draft PR (translation + TLDR + score).
- If the human modifies the AI translation, they must:
  - Update translationHistory with a new entry marking the edit as human-reviewed and set status.translation to AI+Human.
  - Optionally adjust ai_textscore to reflect updated evaluation (could kick off a lightweight re-score pass).
- Reviewer marks the draft PR ready for review and merges into release.

### 6.3 post-release feedback to main

Trigger: push or merge into release.

Purpose: synchronize the approved translations back into main.
Steps:

- Detect new/updated translated files on release not present in main (or with outdated metadata).
- Create a commit directly on main (or via a PR if policy requires) that adds/updates those translations with their final metadata.
- If a translation on release is later modified (e.g., a hotfix), the same mechanism ensures the updated version flows back.

### 6.4 Bad translation detection & issue creation

After translation generation, if review_issues in the AI output is non-empty or ai_textscore.translationQuality is below a threshold:

- Automatically open a GitHub Issue titled: Translation quality issues: <translationKey> -> <targetLang>.
- Body includes: summary, TLDR, list of flagged segments, suggestions, and reference to the draft translation PR.
- Label the issue translation-failure and link it to the translation PR.
- Optionally, the issue can include a suggested regeneration command or link to manual edit path.

### 6.5 Token usage tracking and throttling

Goal: Do not exceed 2 million tokens/day (input + output).

Mechanism:

- Maintain a persistent token usage ledger scoped by date (e.g., translation-metrics/<YYYY-MM-DD>.json) storing cumulative token usage.
- Each translation API response returns usage metadata; the workflow increments the day’s total.
- If the projected total after adding the new usage would exceed the daily cap, the action:
  - Skips further translations for that run.
  - Adds a warning to the summary and optionally opens or updates a translation-rate-limit issue.
- Ledger can live in a dedicated branch or as a committed JSON artifact; use optimistic concurrency (read-modify-write with checks) to avoid race conditions when multiple workflows run in parallel.

---

## 7. Conflict Resolution & Overrides

### 7.1 Conflict rules

- Human-written original content always wins. If an original changes while a translation PR is open and that translation is not yet approved, the pipeline:
  - Marks the existing draft PR as stale (adds label needs-retranslation).
  - Posts a comment explaining the mismatch.
  - Discards or closes the outdated PR (or leaves it for audit) and triggers a fresh translation task with updated sourceSha.

### 7.2 Override / disable automatic translation

- To manually intervene without the automation interfering:
  - Support a repository-level override flag file, e.g., TRANSLATION_PAUSE or a YAML-controlled list like translation.override.yml containing translationKeys or paths to skip.
  - When present for a given content item, the check_translations.js script skips that item, and the CI will not auto-generate its translation.
  - A separate GitHub Action workflow (e.g., manual-translation.yml) can be triggered that creates a manual branch (e.g., manual-translate/<translationKey>) populated with in-progress translation content for editors to modify without AI interference.
  - Removing the flag re-enables the automated pipeline for subsequent pushes.

---

## 8. Metrics, Logging, and Summary

### 8.1 Immediate summaries

Each run emits a digest summary (GitHub Actions summary) containing:

- List of translation tasks (new / stale / skipped) and their reasons.
- TLDRs generated (short previews).
- Text scores (original vs translation).
- Token usage (per translation & cumulative for that CI run).
- Any created or updated draft PRs with links/titles.
- Any issues opened due to quality flags.
- Rate limit or cap-trigger warning if applicable.

### 8.2 Persistent metrics store

Record in a JSON artifact (and optionally commit) the following per run:

- Timestamp, commit SHA, tasks processed count, success/failure per task, tokens consumed, model version, number of PRs created/updated, issues opened, and throttling events.
- A downstream job or manual script can aggregate these over time for trend analysis (e.g., token usage spikes, typical translation quality, backlog of stale translations).

---

## 9. Naming Conventions and Governance

- Branches: translate/<translationKey>-<shortSourceSha>-<lang> (shortSha for brevity, e.g., first 7 of the source commit or sourceSha).
- PR Titles: AI translations: <targetLang> for <translationKey> (source @ <shortSha>)
- Commit messages: AI: <lang> translation for <translationKey> (sourceSha=<shortSha>)
- Labels: ai-translation, needs-review, translation-stale, translation-failure, manual-override
- Issue titles: Translation quality issues: <translationKey> -> <targetLang> or Translation rate limit reached: <date>

---

## 10. Deployment & Release

- Only the release branch is published to GitHub Pages.
- Translation PRs target release; merging implies human approval.
- Post-merge sync updates main to reflect the published state.
- Optional: preview deployments can be configured per draft PR to allow reviewers to sanity-check localized pages before merge.
- Rollback: Revert the translation commit on release and re-sync main accordingly; optionally tag the revert and record its rationale in an audit log (could be a simple markdown changelog entry).

---

## 11. Security & Secret Hygiene

- Store OPENAI_API_KEY in GitHub Secrets; do not log its value.
- Sanitize any echoed prompts or responses in logs to avoid exposing sensitive content if that content is considered private.
- If using a PAT for more granular control over PR creation, limit its scopes to only the necessary contents and pull-requests operations.
- Rotate the key periodically and audit API usage (can be inferred from token usage logs).

---

## 12. Testing & Validation

- Unit tests for the detection and generation scripts using mocked OpenAI responses.
- Schema validation (using a tool like zod or JSON schema) for frontmatter and AI output structure; fail early in CI if invalid.
- Snapshot tests for translated markdown transformations.
- End-to-end integration test that: author → push to main → translation PR appears → merge to release → feed back to main.
- Translation quality regression test: store known good translation pairs and ensure their scores remain within expected ranges or trigger review if degraded.

---

## 13. Step-by-step Implementation Checklist

- Finalize and enforce the extended frontmatter schema with validation.
- Implement helper utilities (SHA computation, language pairing enumeration, override handling).
- Build check_translations.js and verify it detects missing/stale translations and respects overrides.
- Build generate_translations.js with combined prompt logic, parsing, validation, caching, and structured output handling.
- Implement OpenAI token ledger and cap logic.
- Create GitHub Action for main that ties detection + generation + draft PR creation/updating.
- Implement deduplication logic for existing translation PRs.
- Establish manual override workflow.
- Build post-release sync action to feed translations back into main.
- Implement bad translation detection and issue creation.
- Add reviewer checklist and PR template.
- Set up deployment rules (release branch → GitHub Pages).
- Write tests (unit, integration, snapshot).
- Document system operation in repo README / dedicated docs.

---

## 14. Reviewer / Operator Guidance

When reviewing a translation PR, verify:

- TLDR matches intent and content.
- ai_textscore is reasonable relative to historical norms; if very low, consider retranslation.
- Frontmatter updated correctly: language, translationHistory reflects AI or human edits.
- No hallucinated metadata or broken links.
- To force regeneration: add a label like regen-needed or update source content to trigger stale flow.
- To pause automation for a piece: add its key to translation.override.yml or add the TRANSLATION_PAUSE indicator.

---

## 15. Appendix: Example Branch/PR Titles and Commit Messages

- Branch: translate/my-article-abc123d-de
- PR Title: AI translations: de for my-article (source @ abc123d)
- Commit: AI: de translation for my-article (sourceSha=abc123d)
- Issue: Translation quality issues: my-article -> de

---
